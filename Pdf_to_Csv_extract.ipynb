{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6913b577-2dd0-4700-8172-f9e1f4002490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install tabula-py pdfplumber pandas\n",
    "\n",
    "import tabula\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"ğŸš€ Starting Daily PDF to CSV ETL Pipeline...\")\n",
    "\n",
    "# Step 1: Auto-detect LATEST PDF\n",
    "pdf_dir = \"files/aqi/raw\"\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]\n",
    "\n",
    "latest_pdf = None\n",
    "latest_ctime = -1\n",
    "for f in pdf_files:\n",
    "    ctime = os.path.getctime(os.path.join(pdf_dir, f))\n",
    "    if ctime > latest_ctime:\n",
    "        latest_pdf = f\n",
    "        latest_ctime = ctime\n",
    "\n",
    "pdf_path = os.path.join(pdf_dir, latest_pdf)\n",
    "print(f\"ğŸ“„ Processing: {latest_pdf}\")\n",
    "\n",
    "# Extract date from filename\n",
    "yyyymmdd = latest_pdf.split('_')[2].split('.')[0]\n",
    "report_date = datetime.strptime(yyyymmdd, \"%Y%m%d\").date()\n",
    "print(f\"ğŸ“… Report date: {report_date}\")\n",
    "\n",
    "# Step 2: Extract ALL tables\n",
    "dfs = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True, pandas_options={'header': None})\n",
    "print(f\"ğŸ“Š Found {len(dfs)} tables\")\n",
    "\n",
    "# Step 3: Filter AQI tables\n",
    "relevant_dfs = []\n",
    "for i, df in enumerate(dfs):\n",
    "    if df.shape[0] < 3 or df.shape[1] < 6:\n",
    "        continue\n",
    "    sample_text = ' '.join(df.iloc[0:3].astype(str).values.flatten()).lower()\n",
    "    if any(keyword in sample_text for keyword in ['s.no', 'city', 'poor', 'moderate', 'satisfactory']):\n",
    "        print(f\"âœ… Table {i+1} is AQI data: {df.shape}\")\n",
    "        relevant_dfs.append(df)\n",
    "\n",
    "# Step 4: Concatenate & clean\n",
    "aqi_df = pd.concat(relevant_dfs, ignore_index=True)\n",
    "print(f\"ğŸ“ˆ Combined shape: {aqi_df.shape}\")\n",
    "\n",
    "# Step 5: Standardize columns\n",
    "print(\"ğŸ§¹ Cleaning columns...\")\n",
    "aqi_df = aqi_df.iloc[1:].reset_index(drop=True)\n",
    "aqi_df = aqi_df.dropna(how='all')\n",
    "\n",
    "column_mapping = {0: 'SNo', 1: 'City', 2: 'Category', 3: 'AQI_Value', 4: 'Dominant_Pollutant', 5: 'Stations'}\n",
    "\n",
    "if aqi_df.shape[1] == 6:\n",
    "    aqi_df.columns = [column_mapping.get(i, f'Extra_{i}') for i in range(6)]\n",
    "elif aqi_df.shape[1] > 6:\n",
    "    aqi_df = aqi_df.iloc[:, :6]\n",
    "    aqi_df.columns = [column_mapping.get(i, f'Extra_{i}') for i in range(6)]\n",
    "\n",
    "# Clean & add metadata\n",
    "for col in ['SNo', 'City', 'Category', 'Dominant_Pollutant', 'Stations']:\n",
    "    if col in aqi_df.columns:\n",
    "        aqi_df[col] = aqi_df[col].astype(str).str.strip()\n",
    "aqi_df['AQI_Value'] = pd.to_numeric(aqi_df['AQI_Value'], errors='coerce')\n",
    "aqi_df['report_date'] = str(report_date)\n",
    "aqi_df['ingestion_timestamp'] = pd.Timestamp.now()\n",
    "\n",
    "# Filter valid rows\n",
    "aqi_df = aqi_df[\n",
    "    (aqi_df['City'].str.len() > 1) &\n",
    "    (aqi_df['AQI_Value'].notna()) &\n",
    "    (~aqi_df['City'].str.contains('Page|absent|Note|Health', case=False, na=False)) &\n",
    "    (~aqi_df['SNo'].str.contains('nan', na=False))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Cleaned data: {aqi_df.shape} rows\")\n",
    "display(aqi_df.head(10))\n",
    "\n",
    "# Step 6: Save DAILY CSV (One file per day)\n",
    "delta_dir = \"files/aqi/delta/daily\"\n",
    "daily_csv = f\"{delta_dir}/aqi_{yyyymmdd}.csv\"\n",
    "\n",
    "os.makedirs(delta_dir, exist_ok=True)\n",
    "aqi_df['SNo'] = aqi_df['SNo'].astype(str)\n",
    "aqi_df['ingestion_timestamp'] = aqi_df['ingestion_timestamp'].astype(str)\n",
    "aqi_df.to_csv(daily_csv, index=False)\n",
    "\n",
    "print(f\"âœ… Saved TODAY'S data: {daily_csv} ({len(aqi_df)} rows)\")\n",
    "\n",
    "# Step 7: Spark view of ALL daily files\n",
    "all_csvs = [f for f in os.listdir(delta_dir) if f.startswith('aqi_') and f.endswith('.csv')]\n",
    "print(f\"ğŸ“ Found {len(all_csvs)} daily CSV files\")\n",
    "\n",
    "if all_csvs:\n",
    "    combined_df = pd.concat([pd.read_csv(os.path.join(delta_dir, f)) for f in all_csvs], ignore_index=True)\n",
    "    spark_df = spark.createDataFrame(combined_df)\n",
    "    spark_df.createOrReplaceTempView(\"aqi_daily\")\n",
    "    \n",
    "    print(f\"\\nğŸ­ TOP 10 MOST POLLUTED (ALL DAYS):\")\n",
    "    display(spark.sql(\"SELECT * FROM aqi_daily ORDER BY AQI_Value DESC LIMIT 10\"))\n",
    "    \n",
    "    print(f\"ğŸ“Š TOTAL records across {len(all_csvs)} days: {spark_df.count()}\")\n",
    "\n",
    "print(\"\\nğŸ‰ DAILY ETL COMPLETE!\")\n",
    "print(f\"ğŸ“‚ Folder: files/aqi/delta/daily/\")\n",
    "print(f\"ğŸ“Š Query: SELECT * FROM aqi_daily\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pdf_to_Csv_extract",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
