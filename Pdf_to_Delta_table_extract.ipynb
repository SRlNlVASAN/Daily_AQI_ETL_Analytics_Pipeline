{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb3c7eb-5618-41a3-a9dc-5c6f31e3aa4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source - 02_PDF_to_Delta_ETL\n",
    "# Project: India Air Quality Batch ETL Pipeline\n",
    "# Step 2: Extract tables from PDF â†’ Clean DataFrame â†’ Workspace Files (CSV + Parquet)\n",
    "\n",
    "# Install PDF table extraction libraries\n",
    "%pip install tabula-py pdfplumber pandas pyarrow\n",
    "\n",
    "import tabula\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"ğŸš€ Starting PDF to Workspace Files ETL Pipeline...\")\n",
    "\n",
    "# Step 1: Auto-detect LATEST PDF\n",
    "pdf_dir = \"files/aqi/raw\"\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]\n",
    "\n",
    "latest_pdf = None\n",
    "latest_ctime = -1\n",
    "for f in pdf_files:\n",
    "    ctime = os.path.getctime(os.path.join(pdf_dir, f))\n",
    "    if ctime > latest_ctime:\n",
    "        latest_pdf = f\n",
    "        latest_ctime = ctime\n",
    "\n",
    "pdf_path = os.path.join(pdf_dir, latest_pdf)\n",
    "print(f\"ğŸ“„ Processing: {latest_pdf}\")\n",
    "\n",
    "# Extract date from filename\n",
    "yyyymmdd = latest_pdf.split('_')[2].split('.')[0]\n",
    "report_date = datetime.strptime(yyyymmdd, \"%Y%m%d\").date()\n",
    "print(f\"ğŸ“… Report date: {report_date}\")\n",
    "\n",
    "# Step 2: Extract ALL tables\n",
    "dfs = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True, pandas_options={'header': None})\n",
    "print(f\"ğŸ“Š Found {len(dfs)} tables\")\n",
    "\n",
    "# Step 3: Filter AQI tables\n",
    "relevant_dfs = []\n",
    "for i, df in enumerate(dfs):\n",
    "    if df.shape[0] < 3 or df.shape[1] < 6:\n",
    "        continue\n",
    "    sample_text = ' '.join(df.iloc[0:3].astype(str).values.flatten()).lower()\n",
    "    if any(keyword in sample_text for keyword in ['s.no', 'city', 'poor', 'moderate', 'satisfactory']):\n",
    "        print(f\"âœ… Table {i+1} is AQI data: {df.shape}\")\n",
    "        relevant_dfs.append(df)\n",
    "\n",
    "# Step 4: Concatenate tables\n",
    "aqi_df = pd.concat(relevant_dfs, ignore_index=True)\n",
    "print(f\"ğŸ“ˆ Combined shape: {aqi_df.shape}\")\n",
    "\n",
    "# Step 5: Clean CPCB columns\n",
    "print(\"ğŸ§¹ Standardizing columns...\")\n",
    "aqi_df = aqi_df.iloc[1:].reset_index(drop=True)\n",
    "aqi_df = aqi_df.dropna(how='all')\n",
    "\n",
    "column_mapping = {0: 'SNo', 1: 'City', 2: 'Category', 3: 'AQI_Value', 4: 'Dominant_Pollutant', 5: 'Stations'}\n",
    "\n",
    "if aqi_df.shape[1] == 6:\n",
    "    aqi_df.columns = [column_mapping.get(i, f'Extra_{i}') for i in range(6)]\n",
    "elif aqi_df.shape[1] > 6:\n",
    "    aqi_df = aqi_df.iloc[:, :6]\n",
    "    aqi_df.columns = [column_mapping.get(i, f'Extra_{i}') for i in range(6)]\n",
    "\n",
    "# Clean data\n",
    "for col in ['SNo', 'City', 'Category', 'Dominant_Pollutant', 'Stations']:\n",
    "    if col in aqi_df.columns:\n",
    "        aqi_df[col] = aqi_df[col].astype(str).str.strip()\n",
    "aqi_df['AQI_Value'] = pd.to_numeric(aqi_df['AQI_Value'], errors='coerce')\n",
    "aqi_df['report_date'] = str(report_date)\n",
    "aqi_df['ingestion_timestamp'] = pd.Timestamp.now()\n",
    "\n",
    "# Filter valid rows\n",
    "aqi_df = aqi_df[\n",
    "    (aqi_df['City'].str.len() > 1) &\n",
    "    (aqi_df['AQI_Value'].notna()) &\n",
    "    (~aqi_df['City'].str.contains('Page|absent|Note|Health', case=False, na=False)) &\n",
    "    (~aqi_df['SNo'].str.contains('nan', na=False))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… Cleaned data: {aqi_df.shape} rows\")\n",
    "display(aqi_df.head(10))\n",
    "\n",
    "# Step 6: Save to YOUR GitHub-synced Workspace Files (CSV + Parquet)\n",
    "delta_dir = \"files/aqi/delta\"\n",
    "csv_path = f\"{delta_dir}/aqi_daily_all.csv\"\n",
    "parquet_path = f\"{delta_dir}/aqi_daily_all.parquet\"\n",
    "\n",
    "# Create directory\n",
    "os.makedirs(delta_dir, exist_ok=True)\n",
    "\n",
    "# Read existing data (if exists) and append today's data\n",
    "try:\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    print(f\"ğŸ“‚ Found existing data: {len(existing_df)} rows\")\n",
    "    combined_df = pd.concat([existing_df, aqi_df], ignore_index=True)\n",
    "except:\n",
    "    print(\"ğŸ“‚ No existing data, creating new\")\n",
    "    combined_df = aqi_df\n",
    "\n",
    "# Save both formats\n",
    "combined_df.to_csv(csv_path, index=False)\n",
    "combined_df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"âœ… Saved {len(combined_df)} total records:\")\n",
    "print(f\"   ğŸ“„ CSV: {csv_path}\")\n",
    "print(f\"   ğŸª¶ Parquet: {parquet_path}\")\n",
    "\n",
    "# Step 7: Create Spark DataFrame for analysis\n",
    "spark_df = spark.createDataFrame(combined_df)\n",
    "spark_df.createOrReplaceTempView(\"aqi_daily\")\n",
    "\n",
    "# Step 8: Verify TOP polluted cities\n",
    "print(\"\\nğŸ­ TOP 10 MOST POLLUTED:\")\n",
    "display(spark.sql(\"\"\"\n",
    "    SELECT * FROM aqi_daily \n",
    "    ORDER BY AQI_Value DESC \n",
    "    LIMIT 10\n",
    "\"\"\"))\n",
    "\n",
    "print(\"\\nğŸ‰ ETL COMPLETE!\")\n",
    "print(f\"ğŸ“Š Query anytime: SELECT * FROM aqi_daily\")\n",
    "print(f\"ğŸ“‚ GitHub syncs: files/aqi/delta/aqi_daily_all.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pdf_to_Delta_table_extract",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
